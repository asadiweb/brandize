import os
import json
import time
import hashlib
import feedparser
import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator

# -------------------
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# -------------------
FEED_URL = os.getenv("FEED_URL")
ZAPIER_WEBHOOK_URL = os.getenv("ZAPIER_WEBHOOK_URL")  # Catch Hook
POSTED_FILE = os.getenv("POSTED_FILE", "posted_titles.json")
MAX_ITEMS = int(os.getenv("MAX_ITEMS", "5"))  # Ú†Ù†Ø¯ Ù¾Ø³Øª Ø§Ø®ÛŒØ±
SLEEP_BETWEEN = float(os.getenv("SLEEP_BETWEEN", "0.7"))  # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)

if not FEED_URL or not ZAPIER_WEBHOOK_URL:
    raise RuntimeError("FEED_URL ÛŒØ§ ZAPIER_WEBHOOK_URL ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")

translator = GoogleTranslator(source="auto", target="fa")

# -------------------
# ÙØ§ÛŒÙ„ Ø«Ø¨Øª Ø¹Ù†Ø§ÙˆÛŒÙ†/Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡
# -------------------
def load_posted():
    if not os.path.exists(POSTED_FILE):
        return {}
    try:
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            txt = f.read().strip()
            return json.loads(txt) if txt else {}
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† {POSTED_FILE}: {e}")
        return {}

def save_posted(d):
    try:
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(d, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ {POSTED_FILE}: {e}")

# Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¢ÛŒØªÙ… (ØªØ±Ø¬ÛŒØ­Ø§Ù‹ Ù„ÛŒÙ†Ú© ÛŒØ§ id)
def entry_key(entry):
    base = getattr(entry, "id", "") or getattr(entry, "link", "") or getattr(entry, "title", "")
    if not base:
        base = repr(entry)[:200]
    return hashlib.sha1(base.encode("utf-8")).hexdigest()

# -------------------
# Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ HTML: Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ùˆ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† Ù…ØªÙ† Ùˆ ØªØµØ§ÙˆÛŒØ±
# -------------------
def clean_links(html):
    soup = BeautifulSoup(html or "", "html.parser")
    for a in soup.find_all("a"):
        a.unwrap()  # ÙÙ‚Ø· Ù…ØªÙ†Ù Ø¯Ø§Ø®Ù„ Ù„ÛŒÙ†Ú© Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
    return str(soup)

# -------------------
# ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù†
# -------------------
def translate_title(title):
    try:
        return translator.translate(title) if title else title
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù†: {e}")
        return title

# -------------------
# ØªØ±Ø¬Ù…Ù‡ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ø´Ú©Ø³ØªÙ† Ø¨Ù‡ Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ (Ø¨Ø±Ø§ÛŒ Ø¹Ø¨ÙˆØ± Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯ÛŒØª 5000 Ú©Ø§Ø±Ø§Ú©ØªØ±)
# Ø±ÙˆÛŒÚ©Ø±Ø¯: Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ ØªÚ¯â€ŒÙ‡Ø§ (NavigableString) Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
# -------------------
def translate_html_blocks(html):
    soup = BeautifulSoup(html or "", "html.parser")

    # ØªÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ ØªØ±Ø¬Ù…Ù‡ Ø´ÙˆÙ†Ø¯
    skip_tags = {"code", "pre", "script", "style"}
    text_nodes = []

    for elem in soup.find_all(text=True):
        parent = elem.parent.name if elem.parent else ""
        txt = str(elem)
        if parent in skip_tags:
            continue
        # Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§Ù‡Ø§ Ø±Ùˆ Ø±Ø¯ Ú©Ù†
        if not txt.strip():
            continue
        text_nodes.append((elem, txt))

    for node, txt in text_nodes:
        # Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø±Ùˆ Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù/Ø¬Ù…Ù„Ù‡ Ø¨Ø´Ú©Ù†ÛŒÙ…
        parts = split_text_safely(txt, max_len=4000)
        out = []
        for p in parts:
            tr = safe_translate(p)
            out.append(tr)
            time.sleep(SLEEP_BETWEEN)
        node.replace_with("".join(out))

    return str(soup)

def split_text_safely(text, max_len=4000):
    """
    ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù…ØªÙ† Ø±Ø§ Ø§Ø² Ø±ÙˆÛŒ Ø®Ø·ÙˆØ·/Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª <= max_len Ø¨Ø´Ú©Ù†Ø¯.
    """
    if len(text) <= max_len:
        return [text]

    chunks = []
    buf = []
    buf_len = 0

    # Ø§ÙˆÙ„ Ø¨Ø§ \n Ø¨Ø´Ú©Ù†ÛŒÙ…
    paragraphs = text.split("\n")
    for para in paragraphs:
        if not para:
            unit = "\n"
        else:
            unit = para + "\n"

        if buf_len + len(unit) > max_len and buf:
            chunks.append("".join(buf))
            buf = [unit]
            buf_len = len(unit)
        elif len(unit) > max_len:
            # Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯Ù‡Ø› Ø¨Ù‡ Ø¬Ù…Ù„Ù‡ Ø¨Ø´Ú©Ù†
            sentences = split_sentences(unit, limit=max_len)
            for s in sentences:
                if buf_len + len(s) > max_len and buf:
                    chunks.append("".join(buf))
                    buf = [s]
                    buf_len = len(s)
                else:
                    buf.append(s)
                    buf_len += len(s)
        else:
            buf.append(unit)
            buf_len += len(unit)

    if buf:
        chunks.append("".join(buf))
    return chunks

def split_sentences(text, limit=4000):
    # Ø´Ú©Ø³ØªÙ† Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ù‚Ø·Ù‡/Ø¹Ù„Ø§Ù…Øª ØªØ¹Ø¬Ø¨/Ø³Ø¤Ø§Ù„
    import re
    sents = re.split(r'([\.!\?]+[\s]*)', text)
    merged = []
    for i in range(0, len(sents), 2):
        core = sents[i]
        tail = sents[i+1] if i+1 < len(sents) else ""
        merged.append(core + tail)

    chunks = []
    cur = ""
    for s in merged:
        if len(cur) + len(s) > limit and cur:
            chunks.append(cur)
            cur = s
        else:
            cur += s
    if cur:
        chunks.append(cur)
    return chunks

def safe_translate(text):
    try:
        return translator.translate(text)
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø¨Ù„ÙˆÚ©: {e}")
        return text

# -------------------
# Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Zapier Webhook (Ú©Ù‡ Ø§ÙˆÙ†Ø¬Ø§ Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ù‡ Blogger Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
# -------------------
def send_to_zapier(title_fa, html_fa, source_url=None, published=None):
    payload = {
        "title": title_fa,
        "html": html_fa,            # Ø¨Ø¯Ù†Ù‡ HTML Ú©Ø§Ù…Ù„
        "source_url": source_url or "",
        "published": str(published or ""),
    }
    try:
        r = requests.post(ZAPIER_WEBHOOK_URL, json=payload, timeout=30)
        if 200 <= r.status_code < 300:
            print(f"âœ… Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Zapier Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {title_fa}")
            return True
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Zapier [{r.status_code}]: {r.text[:300]}")
        return False
    except Exception as e:
        print(f"âŒ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Zapier: {e}")
        return False

# -------------------
# Ø§Ø¬Ø±Ø§
# -------------------
def main():
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÛŒØ¯ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Zapier")
    posted = load_posted()

    feed = feedparser.parse(FEED_URL)
    if not feed.entries:
        print("âš ï¸ Ù‡ÛŒÚ† Ù…Ø·Ù„Ø¨ÛŒ Ø¯Ø± ÙÛŒØ¯ Ù†ÛŒØ³Øª.")
        return

    count = 0
    for entry in feed.entries[:MAX_ITEMS]:
        key = entry_key(entry)
        if key in posted:
            print(f"â© Ø±Ø¯Ù ØªÚ©Ø±Ø§Ø±ÛŒ: {getattr(entry, 'title', '')[:80]}")
            continue

        raw_title = getattr(entry, "title", "").strip()
        raw_html = getattr(entry, "summary", "") or getattr(entry, "description", "")
        raw_html = raw_html or ""

        # 1) Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        html_no_links = clean_links(raw_html)

        # 2) ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…Ø­ØªÙˆØ§ (Ø¨Ø§ Ø¨Ù„ÙˆÚ©â€ŒØ¨Ù†Ø¯ÛŒ Ø§ÛŒÙ…Ù†)
        title_fa = translate_title(raw_title)
        html_fa = translate_html_blocks(html_no_links)

        # 3) Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Zapier (Zap Ø¢Ù† Ø±Ø§ Ø§ÛŒÙ…ÛŒÙ„ HTML Ø¨Ù‡ Ø¢Ø¯Ø±Ø³ @blogger.com Ù…ÛŒâ€ŒÙØ±Ø³ØªØ¯)
        ok = send_to_zapier(
            title_fa=title_fa,
            html_fa=html_fa,
            source_url=getattr(entry, "link", ""),
            published=getattr(entry, "published", ""),
        )
        if ok:
            posted[key] = {
                "title_de": raw_title,
                "title_fa": title_fa,
                "link": getattr(entry, "link", ""),
                "sent_at": int(time.time()),
            }
            save_posted(posted)
            count += 1
            time.sleep(0.5)  # Ú©Ù…ÛŒ ØªÙ†ÙØ³ Ø¨ÛŒÙ† Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§

    print(f"ğŸ Ù¾Ø§ÛŒØ§Ù†. Ø§Ø±Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯: {count}")

if __name__ == "__main__":
    main()
